Is an automatic or manual transmission better for MPG ?
========================================================

In this study, we are paricularly interested in answering the two questions below

* Is an automatic or manual transmission better for miles/(US) gallon (MPG) ?
* Quantifying how different is the MPG between automatic and manual transmissions?

Our dataset `mtcars` contains **`r nrow(mtcars)`** observations on  the following **`r ncol(mtcars)`** variables.
* [, 1]  `mpg`	 Miles/(US) gallon
* [, 2]	 `cyl`	 Number of cylinders
* [, 3]	 `disp`	 Displacement (cu.in.)
* [, 4]	 `hp`	 Gross horsepower
* [, 5]	 `drat`	 Rear axle ratio
* [, 6]	 `wt`	 Weight (lb/1000)
* [, 7]	 `qsec`	 1/4 mile time
* [, 8]	 `vs`	 V/S
* [, 9]	 `am`	 Transmission (0 = automatic, 1 = manual)
* [,10]	 `gear`	 Number of forward gears
* [,11]	 `carb`	 Number of carburetors

We can frame this into a **two-sided statistical hypothesis test** :

$H_O$ : $\beta_1 = 0$. The true linear model has the slope zero for `am`, which means that the transmission mode does not relate to the number of miles per gallon of the car.

$H_A$ : $\beta_1 \neq 0$. The true linear model has the slope different from zero for `am`. If $\beta_1$ is positive, it means manual transmission can have $\beta_1$ **more** mpg in comparison with automatic mode. If $\beta_1$ is negative, the manual transmission will have $\beta_1$ **less** mpg than automatic mode.


## Model Selection

Let's first do a multivarible regression as following
```{r}
summary(lm(mpg ~ ., data = mtcars))$coefficients
```
We will then use the **backward-elimination** strategy to eliminate the unrelated variables one-at-a-time. It means, we first fit a model which includes all the potential variables as above, now we drop the variable `cyl` as it has the largest p-value, then we refit the model. All the details of the model selection is attached in Appendix A.

In the new model, there is no strong evidence that the coefficient of the variable `vs` is different from zero even though its p-value decreased a little bit, so we again eliminate the variable with the largest p-value `vs` and refit the model

With the same strategy, we can now eliminate the variable largest p-value as `carb`, `gear`, `drat` `disp`, `hp` and the intercept in order and refit the model. The final model is
```{r}
fit <- lm(mpg ~ wt + qsec + am - 1, data = mtcars)
summary(fit)$coefficients
```
It includes only `wt`, `qsec` and `am` in predicting the miles per gallon of a vehicle
$$
\hat y = `r fit$coef["qsec"]` x_{wt} + `r fit$coef["qsec"]` x_{qsec} + `r fit$coef["am"]` x_{am}
$$


-----------
## Appendix A. Model Selection details
```{r}
dat <- mtcars
dat <- dat[, names(dat) != "cyl"]
summary(lm(mpg ~ ., data = dat))$coefficients
dat <- dat[, names(dat) != "vs"]
summary(lm(mpg ~ ., data = dat))$coefficients
dat <- dat[, names(dat) != "carb"]
summary(lm(mpg ~ ., data = dat))$coefficients
dat <- dat[, names(dat) != "gear"]
summary(lm(mpg ~ ., data = dat))$coefficients
dat <- dat[, names(dat) != "drat"]
summary(lm(mpg ~ ., data = dat))$coefficients
dat <- dat[, names(dat) != "disp"]
summary(lm(mpg ~ ., data = dat))$coefficients 
dat <- dat[, names(dat) != "hp"]
summary(lm(mpg ~ ., data = dat))$coefficients 
summary(lm(mpg ~ . -1, data = dat))$coefficients 
```
As the p-value of all the remaining predictors are smaller than $0.05$, we can stop.